# -*- eval: (let () (org-babel-goto-named-src-block "setup-elisp-env") (org-babel-execute-src-block)); -*-


* VAE Project

* Purpose
  Test out if it is possible to make VAE's each learn to recognize a different
  object.

  For example, let us consider the MNIST dataset restricted only to the digits 0
  and 1.
  We would like to have 2 VAE's, and have each one of them learn to represent the
  digit 0, and the other one learn to represent the digit 1.


* Project structure
** ~generate.py~ -- once a model has been trained, use this to sample images.
** ~train.py~ -- run this in order to train the model.
** ~vae.py~ -- this is the base variational autoencoder model/
** ~supervae.py~ -- this is the model which contains multiple vae's inside, each of
   which is supposed to learn a different concept.


* Tasks
** DONE Implement Beta-VAE.
   CLOSED: [2019-03-12 Tue 17:49]
** DONE Figure out why running on two datasets causes the error.
   CLOSED: [2019-03-11 Mon 14:35]
   `tf.function-decorated function tried to create variables on non-first call.`

** DONE Implement entropy loss.
   CLOSED: [2019-03-15 Fri 18:26]
*** Do a pixel-wise entropy function (i.e., for each pixel consider that the VAE's give a probability distribution, and compute the entropy for each pixel).
*** Then take the sum of all entropies, and add Alpha * log(S) to the loss function, where Alpha is a hyperparameter.
*** Keep in mind that maybe we have to normalize the Entropy term depending on the number of VAE's.
** DONE Implement VAE's for 3 digits.
   CLOSED: [2019-04-15 Mon 15:59]
** DONE Figure out what is the best way to freeze a VAE
   CLOSED: [2019-03-15 Fri 18:25]
*** One way would be to make the learning rate very small, so that nothing changes.



* Solved issues
** The confidences for both models always look like coffee beans.
   Is this ok?
   A: If you decrease the entropy loss coefficient, the confidences start becoming more descriptive.
** VAE_1 learns both 0 and 1.
   This seems to be unavoidable, as encoding the digit iself takes only 1 bit.
*** Maybe an idea would be to make the models so weak, that they are unable to
    learn two digits at the same time.
    A: This does not work, because weak models just split the work of drawing a 0.
** Make VAE_1 learn a residual between the input picture and what VAE_0 predicts.
   This should have the effect that it does not even have the chance to learn what a 0 is.
   A: This is not that useful, because residuals impose a sequence-dependence on the order of learning.

** Model almost working!
*** Gamma=0.005
    ~! python3 train.py --name colab --beta 1 --gamma 0.005 --epochs 40 80 --latent_dim 8 --nlayers 3~
    VAE_1 seems to 'barely' learn anything about zeroes, in that it draws much in a much uglier way than the 1's.
    Furthermore, the confidences of VAE_1 for the zeroes are very very low (almost black).
    This might mean two things:
    1) The entropy loss is a little bit too high, and so VAE_1 is forced to learn about zeroes only to insure that
       there is not too much entropy loss incurred.
    2) VAE_1 has to much available entropy, and decided to spend some of it on the wrong digit.
    #+CAPTION: Initial progress
    #+attr_html: :width 700px
    [[file:./_org_res/init_progress.png]]
*** Gamma=0.0002
    When gamma is too small, VAE_0 has very high confidences where there is a 0, as well as where there is nothing.
    In accordance, VAE_1 either predicts 1's where they actually exist, or it puts a very low confidence very generic 0
    everywhere else.
    ~! python3 train.py --name colab --beta 1 --gamma 0.0002 --epochs 40 80 --latent_dim 8 --nlayers 3~
    #+CAPTION: Gamma too small
    #+attr_html: :width 700px
    [[file:./_org_res/big_gamma_init_progress.png]]

** Make ReLU after BN and fix on this architecture.

** Try to fully freeze models.
   I.e., not apply any training at all.

** Try to let VAE_0 train for longer.
   Also plot how it fares when dealing with pictures of 0 and 1, to see what happens.
** Try to see if there are bugs.
   Maybe VAE_0 is not properly frozen.

** Network params results
   These are all done with ReLU before BN, with no FC.
  | Beta |  Gamma | Good/(Good+Bad) | Obs.                     |
  |------+--------+-----------------+--------------------------|
  |  1.0 | 0.0005 | 1/2             | In bad, VAE_0 dominates. |
  |  2.0 |  0.001 | 2/5             | In bad, VAE_0 dominates. |
  |  2.0 |  0.002 | 1/4             |                          |
  |  2.0 |  0.005 | 3/6             | In bad, VAE_1 collapses. |
  |  1.0 | 0.0001 | 0/2 bad         | All white.               |
  |  1.0 |  0.001 | 0/2 bad         |                          |
  |  1.0 |  0.005 | 0/2 bad         |                          |
  |  2.0 | 0.0001 | 0/2 bad         | All white.               |
  |  2.0 | 0.0005 | 0/2 bad         | VAE_0 too confident      |

  ReLU before BN, with FC (but no activation):
 | Beta | Gamma | Good/(Good+Bad) | Obs.                       |
 |------+-------+-----------------+----------------------------|
 |    2 | 0.001 | 3/4             | When bad, VAE_0 dominates. |
 |    2 | 0.002 | 0/4             |                            |
 |    2 | 0.005 | 3/4             |                            |


** Network Architecture results
    | ReLU / SeLU | Act. before/after BN | FC at end | Works?      |
    |-------------+----------------------+-----------+-------------|
    | ReLU        | Before               | No        | 5 Yes, 0 No |
    | ReLU        | Before               | Yes       | 3 Yes, 1 No |
    | SeLU        | No BN                | Yes       | Yes         |
    | SeLU        | No BN                | No        | Yes         |
    | ReLU        | After                | No        | No          |
    | ReLU        | After                | Yes       | No          |
    | ReLU        | No                   | ?         | No          |

    As a result, there is a single architecture which seems most likely to work:
    ReLU, act. before BN, no FC at the end.

** VAE_1 may collapse.
   Occasionally, VAE_1 will not learn anything. As soon as it starts training, its KL-loss becomes 0 and stays 0.
   This may be because the KL loss for VAE_0 will be fixed and cannot change, and hence maybe not much is left
   over for VAE_1.

   One issue: depending on \Beta, maybe VAE_0 "gobbles up" all of the available information.
   This way, when VAE_1 starts learning, it cannot learn anything because doing so would
   incur a pretty hefty KL-loss penalty.

   See this paper https://arxiv.org/pdf/1808.04947.pdf for possible solutions.

** Vanishing gradients
   It seems that when training multiple VAE's, eventually we run into the problem of vanishing gradients.
   Possible solutions: different activations?

** Collapsing becomes a big issue when training with multiple VAE's.
   Since it happens randomly also with only 2, that should hopefully be solved before we start doing anything else.

** Add batch normalization to the confidence values for each VAE.
   This way all VAE's will produce confidences within the same ballpark values, so there is no more overpowering by the early
   VAE's who get a chance to up their confidences really really high.

** Plot KL for fixed image, to see if it activated or not.
** Scale to harder problems:

*** Have more digits.

** Try to see why the KL of VAE-0 is higher when there is no 0.
** Try to train all VAE's together at the end for some time, with a lower lr.
*** Try to also increase KL loss for this scenario.
** Try to maybe also feed empty blocks very bright.
** List of hyperparams which produce satisfactory results:
#+NAME: setup-elisp-env
#+BEGIN_SRC elisp :results silent
    (defun run-experiment-with-params (root-dir block-name)
    (setq root_dir root-dir)
    (save-excursion
      (goto-char
        (org-babel-find-named-block block-name))
      (org-babel-execute-src-block-maybe))
    )
  (setq digits "44")
#+END_SRC

  #+NAME: generate-table-with-links
  #+BEGIN_SRC python :var table=good-hyperparam-table :var base_dir="nvaes=5_separate_training" :results value :colnames no :hlines yes
    for row in table[1:]:
        if row is None:
          continue
        beta = row[0]
        gamma = row[1]
        run = row[2]
        root_dir = f'../_save/{base_dir}/beta={beta}_gamma={gamma}/run-{run}'
        row[3] = f'[[elisp:(run-experiment-with-params "{root_dir}" "sample-experiment")][click]]'
    return table
  #+END_SRC

  #+NAME:good-hyperparam-table
  #+RESULTS: generate-table-with-links(table=good-hyperparam-table)
  | Beta | Gamma | Run | Link  |
  |------+-------+-----+-------|
  |  0.5 | 0.007 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=5_separate_training/beta=0.5_gamma=0.007/run-1" "sample-experiment")][click]] |
  |  0.5 |  0.01 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=5_separate_training/beta=0.5_gamma=0.01/run-1" "sample-experiment")][click]] |
  |  0.5 |  0.02 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=5_separate_training/beta=0.5_gamma=0.02/run-1" "sample-experiment")][click]] |
  |  0.7 | 0.007 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=5_separate_training/beta=0.7_gamma=0.007/run-1" "sample-experiment")][click]] |
  |  0.7 |  0.01 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=5_separate_training/beta=0.7_gamma=0.01/run-1" "sample-experiment")][click]] |
  |  0.7 |  0.02 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=5_separate_training/beta=0.7_gamma=0.02/run-1" "sample-experiment")][click]] |

  #+NAME: generate-table-with-links(table=good-hyperparam-table)
  #+CALL: generate-table-with-links(table=good-hyperparam-table, base_dir="nvaes=5_separate_training")




 #+NAME: sample-experiment
 #+BEGIN_SRC sh :var root_dir=(identity root_dir) digits=(identity digits)
   python3 sample.py --name leonhard --digits "${digits}" --root-dir "${root_dir}" --num-examples 4 --epoch 'latest'
 #+END_SRC

 #+RESULTS: sample-experiment
 | (4, | 28, | 56, | 1)         |           |            |            |          |   |
 | KL  | for |  0: | [1.2666323 |  5.174501 | 0.99687195 |  1.4980268 | ]        |   |
 | KL  | for |  1: | [3.2226562 | 1.2623823 |  2.0987422 |   2.360978 | ]        |   |
 | KL  | for |  2: | [18.318132 | 17.039242 |  19.780178 | 18.722046] |          |   |
 | KL  | for |  3: | [          | 5.0307403 |  6.3076477 |  12.696253 | 9.141944 | ] |


** Sometimes later models still collapse
 #+NAME: collapse-hyperparam-table
 | Beta | Gamma |
 |------+-------|
 |  0.7 | 0.005 |

 #+CALL: generate-table-with-links(table=collapse-hyperparam-table)

 #+RESULTS:
 | 0.7 | 0.005 | [[elisp:(run-experiment-with-params 0.7 0.005)]] |

** Sometimes KL does not change if model is doing something
#+NAME: kl-does-not-change-hyperparam-table
 | Beta | Gamma |
 |------+-------|
 |  0.9 | 0.005 |
 In this case, this happens for VAE-3.

#+CALL: generate-table-with-links(table=kl-does-not-change-hyperparam-table)

#+RESULTS:
| 0.9 | 0.005 | [[elisp:(run-experiment-with-params 0.9 0.005)]] |



* Current issues
** Maybe switch to 3D and then try less supervision
   It seems that most approaches which perform well on MNIST do not actually generalize well to other approaches.
   With that in mind, it might be better to first transition to a more realistic dataset, and only then try to
   achieve "true" supervision.
*** Train all at once, and the ones with high KL loss get frozen.
** Results for training all together

   VAE for 4 does not seem to learn anything: VAE_2 learns the digit 3 as well as the digit 2.
   Try this with [[elisp:(setq digits "33")][digits set to 33]].
   #+NAME: train-together-table-bad
   #+RESULTS: update-train-together-table-bad
   | Beta | Gamma | Run | Link  |
   |------+-------+-----+-------|
   |  0.5 | 0.005 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.5_gamma=0.005/run-1" "sample-experiment")][click]] |
   |  0.5 | 0.005 |   2 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.5_gamma=0.005/run-2" "sample-experiment")][click]] |
   |  0.7 | 0.005 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.7_gamma=0.005/run-1" "sample-experiment")][click]] |
   |  0.7 | 0.005 |   2 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.7_gamma=0.005/run-2" "sample-experiment")][click]] |
   |  0.7 | 0.007 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.7_gamma=0.007/run-1" "sample-experiment")][click]] |
   |  0.7 | 0.007 |   2 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.7_gamma=0.007/run-2" "sample-experiment")][click]] |
   |------+-------+-----+-------|
   |  0.7 |  0.01 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.7_gamma=0.01/run-1" "sample-experiment")][click]] |
   |  0.5 | 0.007 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.5_gamma=0.007/run-1" "sample-experiment")][click]] |
   |  0.5 |  0.01 |   1 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.5_gamma=0.01/run-1" "sample-experiment")][click]] |

   #+NAME: update-train-together-table-bad
   #+CALL: generate-table-with-links(table=train-together-table-bad, base_dir="nvaes=4_train_together")

   VAE's seem to perform well, but only on certain runs (not all):

   #+NAME: train-together-table-well
   #+RESULTS: update-train-together-table-well
   | Beta | Gamma | Run | Link  |
   |------+-------+-----+-------|
   |  0.7 |  0.01 |   2 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.7_gamma=0.01/run-2" "sample-experiment")][click]] |
   |  0.5 | 0.007 |   2 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.5_gamma=0.007/run-2" "sample-experiment")][click]] |
   |  0.5 |  0.01 |   2 | [[elisp:(run-experiment-with-params "../_save/nvaes=4_train_together/beta=0.5_gamma=0.01/run-2" "sample-experiment")][click]] |

   #+NAME: update-train-together-table-well
   #+CALL: generate-table-with-links(table=train-together-table-well, base_dir="nvaes=4_train_together")

* Stashed ideas
** Decrease Gamma and increase the KL-loss while training VAE_1,
   in order to encourage it to learn one single thing, and learn it well.
   However, since VAE_0 is not learning anything anymore, maybe we should also decrease the KL-loss weight.
** After VAE_0 has learned its digit, find out the KL loss. Then try to force VAE_1 to have
   a similar KL loss, by using the Beta-VAE paper trick.
** Add loss for generating images: if you decide to output a non-trivial pixel, then you should
   be very confident in your prediction.
   Another idea in a similar fashion: if you output an image but you have low confidence, don't even bother.
** Divide the KL-loss by the number of active VAE's.
   This way, when we active more of them, the new ones still have leftover capacity.
** Add FC to end of encoder.
   It seems that most models for CNN's have two FC's at the end: one with arbitrary size and one for the latent dimension.
   It might be wise to also augment our model with two FC's instead of a single one.


* Outstanding ideas

* Next issues
